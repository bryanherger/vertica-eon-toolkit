We created these scripts to help during a POC, ideally you can install these scripts in your Management Console instance. It uses the aws CLI so you need to configure it beforehand, you only need to set the region.

Make sure you try this in a test environment. The scripts don't validate all the different errors.

The scripts are not super clean, but they work.

aws.sh, basic environment settings:

VSQL_PASSWORD, password for database, I assume the user is dbadmin

VSQL_HOST, IP address for one of the nodes in the DB, will be used for ssh commands and for vsql

VSQL_DATABASE, database name

SUBNET, AWS subnet ID, you need to get this from one of the running instances in the cluster

S3_PATH, path for S3 communal storage

SECURITY_GROUP, AWS security group, get this from on of the instances in the cluster

AWS_INSTANCE_TYPE, AWS instance type you want to use

IAM_INSTANCE_PROFILE, role you want to use, again get it from an instance in the cluster

AWS_IMAGE_ID, Vertica AIM id, these change with every release and hotfix

USER_DATA, path for the startup script to use when an instances first starts, this script needs to be updated to adjust the authorized ssh keys.

AWS_KEY_NAME, ssh key for instances (the one from AWS)

AWS_TAGS, tags you want to use for your instances

SSH_KEY, path for the ssh key to connect to nodes


The scripts:
sleepDB.sh, will hibernate the database and stop the instances. No parameters needed.

Example: ./sleepDB.sh


wakeupDB.sh, start the instances and revive DB, had to revive because I was using i3 instances,  all the data was gone after stopping the instances. No parameters needed, will us a file generated by sleepDB.sh

Example: ./wakeupDB.sh


warmUp.sql, file with explain local queries to warm-up the cache after waking up the database. Will run them on all nodes.

expandCluster.sh, first parameter is the number of instances you want to add and an optional second parameter to create a fault group and add the new instances to it

Example: ./expandCluster.sh 3 fg2


shrinkCluster.sh needs a list of the IP addresses for the nodes you want to remove, separated by comma. It will remove the nodes from the DB, cluster and terminate the instances.

Example: ./shrikCluster.sh 10.135.200.100,10.135.200.24,10.135.200.100.38


userData.sh, update the line with the ssh keys, one of them has to be the one you use to log in to the instances, then you can add the one from the MC and finally add the ones that was generated during installation for one of the existing nodes.

